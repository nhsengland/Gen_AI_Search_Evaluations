{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM as a Judge\n",
    "\n",
    "LLM as a Judge is a method for evaluating summaries with the use of an LLM.\n",
    "\n",
    "Typically this can be done in one of two ways:\n",
    "\n",
    "1 - An LLM judges a single summary based on a schema, which may allow for comparisons to a 'ground truth' summary or to the reference material. \n",
    "\n",
    "2 - An LLM compares two summaries, and judges which one is better based on a certain characteristic.\n",
    "\n",
    "This notebook will give run through two examples exploring LLM as a Judge.\n",
    "\n",
    "The aim of this notebook is to help explain the theory behind LLM as a Judge. See `4 - Further Considerations` before considering using LLM as a Judge for your own evaluations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Summary Generation\n",
    "\n",
    "First, let us create two summaries to use for the example.\n",
    "\n",
    "We will be creating two summaries from the `nhs_history.txt` file, based on [this](https://www.england.nhs.uk/nhsbirthday/about-the-nhs-birthday/nhs-history/) page from the NHS England website.\n",
    "\n",
    "We shall use GPT4 hosted on Azure to generate one summary, and we will manually change another summary to have a few inaccuracies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our LLM endpoint url and key saved in a .env file.\n",
    "\n",
    "ENDPOINT = os.getenv(\"ENDPOINT_URL\")\n",
    "API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"api-key\": API_KEY,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define our prompt to send to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../example_data/nhs_history.txt\", \"r\") as file:\n",
    "    data = file.read()\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You are going to provide a short one paragraph summary response given some DATA and a QUESTION. \n",
    "\n",
    "The DATA is: {data}\n",
    "\n",
    "The QUESTION is: What key events occurred in the NHS between 1990 and the year 2000?\n",
    "\n",
    "Only include in your response information directly from the DATA.\n",
    "Only respond with the summary, do not include anything else in your response.\n",
    "\n",
    "RESPONSE:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we generate a payload, which formats the prompt alongside other information such as the temperature into the correct format for calling the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payload for the request\n",
    "payload = {\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": \"You are an AI assistant that helps people find information.\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": prompt\n",
    "    }\n",
    "  ],\n",
    "  \"temperature\": 0.7,\n",
    "  \"top_p\": 0.95,\n",
    "  \"max_tokens\": 800\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then send the request to the API and get the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send request\n",
    "try:\n",
    "    response = requests.post(ENDPOINT, headers=headers, json=payload)\n",
    "    response.raise_for_status()  # Will raise an HTTPError if the HTTP request returned an unsuccessful status code\n",
    "except requests.RequestException as e:\n",
    "    raise SystemExit(f\"Failed to make the request. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_response = response.json()\n",
    "summary_1 = json_response[\"choices\"][0][\"message\"][\"content\"]\n",
    "print(summary_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our second summary, we wil add some changes.\n",
    "\n",
    "- We will add a fact which does not appear in the data - \"In 1993, doctors held a massive tea party in London to celebrate a successful knee transplant.\"\n",
    "- We will add a spelling mistake. \"syndrome\" -> \"sindrome\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_2 = \"Between 1990 and 2000, key NHS events included the introduction of a vaccine against Haemophilus influenzae type B (Hib) in 1992, which combats childhood meningitis, and the world's first laser surgery on babies for twin-to-twin transfusion sindrome. In 1993, doctors held a massive tea party in London to celebrate a successful knee transplant. The NHS Organ Donor Register was established in 1994. In 1999, the UK became the first to use a vaccine against Group C meningococcal disease. In 2000, NHS walk-in centres were introduced to provide easy access to various services.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us save both summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../example_data/summary_1.txt\", \"w\") as text_file:\n",
    "    text_file.write(summary_1)\n",
    "with open(\"../example_data/summary_2.txt\", \"w\") as text_file:\n",
    "    text_file.write(summary_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Example 1: LLM as a Judge for a Single Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check for the groundedness of a response.\n",
    "\n",
    "Groundedness asserts that each sentence in the summary is supported by sentences in the reference data.\n",
    "\n",
    "It is a metric that can be used in [vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/grounding/overview) and [Azure AI Studio](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/quickstart-groundedness?tabs=curl).\n",
    "\n",
    "In this example we will craft a prompt ourselves to bette understand how we can measure groundedness.\n",
    "\n",
    "We follow good prompting guidance by:\n",
    "- Giving clear scoring criteria.\n",
    "- Giving examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../example_data/nhs_history.txt\", \"r\") as file:\n",
    "    data = file.read()\n",
    "with open(\"../example_data/summary_1.txt\", \"r\") as file:\n",
    "    summary_1 = file.read()\n",
    "with open(\"../example_data/summary_2.txt\", \"r\") as file:\n",
    "    summary_2 = file.read()\n",
    "\n",
    "groundedness_prompt = \"\"\"\n",
    "You are a judge evaluating the groundedness of a summary based on a set of supporting documents. For a summary to be grounded, each sentence must have clear support within the provided documents. Please assess each sentence in the summary, assign a score between 0 and 5 based on how well it is grounded in the supporting documents, and explain your reasoning.\n",
    "\n",
    "Scoring Criteria\n",
    "0: No grounding; sentence has no support in the documents.\n",
    "1-2: Partially grounded; weak or incomplete support, or indirect references.\n",
    "3-4: Mostly grounded; strong support but some ambiguity or indirectness.\n",
    "5: Fully grounded; sentence has direct, clear support in the documents.\n",
    "Instructions\n",
    "Analyze each sentence in the summary and determine if it is grounded in the supporting_documents.\n",
    "Score each sentence from 0 to 5, referencing the relevant supporting document(s) as evidence.\n",
    "Explain your reasoning for each score, identifying specific references in the documents.\n",
    "Provide an overall groundedness score for the entire summary, based on the individual scores.\n",
    "Format Your Response as Follows:\n",
    "\n",
    "\"{{\n",
    "  \"summary\": \"<summary text>\",\n",
    "  \"supporting_documents\": \"[<list of supporting documents>]\",\n",
    "  \"sentence_scores\": [\n",
    "    {{\n",
    "      \"sentence\": \"<sentence from summary>\",\n",
    "      \"score\": \"<score from 0 to 5>\",\n",
    "      \"justification\": \"<justification for score>\"\n",
    "    }},\n",
    "    ...\n",
    "  ],\n",
    "  \"overall_groundedness_score\": \"<average score>\"\n",
    "}}\"\n",
    "\n",
    "Example\n",
    "Given the following summary and documents:\n",
    "\n",
    "Summary: \"The LLM is effective at summarization, with high precision on factual data.\"\n",
    "Supporting Documents:\n",
    "Document 1: \"The LLM achieves high precision in tasks involving factual summarization.\"\n",
    "Document 2: \"In multiple benchmarks, the LLM demonstrated effective summarization.\"\n",
    "Your response might look like this:\n",
    "\n",
    "{{\n",
    "  \"summary\": \"The LLM is effective at summarization, with high precision on factual data.\",\n",
    "  \"supporting_documents\": [\n",
    "    \"The LLM achieves high precision in tasks involving factual summarization.\",\n",
    "    \"In multiple benchmarks, the LLM demonstrated effective summarization.\"\n",
    "  ],\n",
    "  \"sentence_scores\": [\n",
    "    {{\n",
    "      \"sentence\": \"The LLM is effective at summarization.\",\n",
    "      \"score\": 5,\n",
    "      \"justification\": \"This sentence is directly supported by Document 2.\"\n",
    "    }},\n",
    "    {{\n",
    "      \"sentence\": \"It has high precision on factual data.\",\n",
    "      \"score\": 4,\n",
    "      \"justification\": \"Mostly supported by Document 1, though precision could be more explicitly mentioned.\"\n",
    "    }}\n",
    "  ],\n",
    "  \"overall_groundedness_score\": 4\n",
    "}}\n",
    "Your Task:\n",
    "Now, please apply this approach to the provided summary and documents below.\n",
    "\n",
    "Summary: {summary}\n",
    "Supporting Documents: {data}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to before, we format our prompt into a payload.\n",
    "\n",
    "You may notice the temperature has been reduced in comparison to when we generated the summary. This is typical for LLM as a Judge - we want the model to be less creative with it's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payload for the request\n",
    "payload = {\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": \"You are an AI assistant that acts as a judge.\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": groundedness_prompt.format(summary=summary_1, data=data)\n",
    "    }\n",
    "  ],\n",
    "  \"temperature\": 0.2,\n",
    "  \"top_p\": 0.95,\n",
    "  \"max_tokens\": 800\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we send summary one to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send request\n",
    "try:\n",
    "    response = requests.post(ENDPOINT, headers=headers, json=payload)\n",
    "    response.raise_for_status()  # Will raise an HTTPError if the HTTP request returned an unsuccessful status code\n",
    "except requests.RequestException as e:\n",
    "    raise SystemExit(f\"Failed to make the request. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_response = response.json()\n",
    "summary_1_rating = json_response[\"choices\"][0][\"message\"][\"content\"]\n",
    "print(summary_1_rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the response, we see that the LLM has correctly identified that each sentence in the summary is supported by documentation.\n",
    "\n",
    "Next, let us check summary 2.\n",
    "\n",
    "**Remember, this summary has a sentence added which is not from the supporting documentation.** We expect this to be detected by the LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payload for the request\n",
    "payload = {\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": \"You are an AI assistant that acts as a judge.\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": groundedness_prompt.format(summary=summary_2, data=data)\n",
    "    }\n",
    "  ],\n",
    "  \"temperature\": 0.2,\n",
    "  \"top_p\": 0.95,\n",
    "  \"max_tokens\": 800\n",
    "}\n",
    "\n",
    "# Send request\n",
    "try:\n",
    "    response = requests.post(ENDPOINT, headers=headers, json=payload)\n",
    "    response.raise_for_status()  # Will raise an HTTPError if the HTTP request returned an unsuccessful status code\n",
    "except requests.RequestException as e:\n",
    "    raise SystemExit(f\"Failed to make the request. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_response = response.json()\n",
    "summary_1_rating = json_response[\"choices\"][0][\"message\"][\"content\"]\n",
    "print(summary_1_rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this response, we see that the LLM has correctly identified that one of the sentences is not supported in the documentation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Example 2: LLM as a Judge for Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pairwise Comparisons is a method of evaluating LLM responses by comparing two responses, and voting which is best. Similar to above, where we chose a specific metric (groundedness), we can pick a specific metric for pairwise comparison. \n",
    "\n",
    "In this example, we will use fluency as an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluency_prompt = \"\"\"\n",
    "You are a judge evaluating the fluency of two summaries. Fluency here refers to how smoothly and naturally each summary reads, how well-constructed its sentences are, and whether it has a clear and cohesive flow. Please compare the two summaries below and select the one you find more fluent.\n",
    "\n",
    "Instructions:\n",
    "Analyze both summaries and consider which one reads more naturally and cohesively.\n",
    "Choose the more fluent summary and provide a brief explanation if necessary.\n",
    "Focus on sentence structure, clarity, and flow rather than content accuracy or completeness.\n",
    "Format Your Response as Follows:\n",
    "{{\n",
    "  \"more_fluent_summary\": \"<Summary A or Summary B>\",\n",
    "  \"explanation\": \"<Optional explanation for why the selected summary is more fluent>\"\n",
    "}}\n",
    "Summaries:\n",
    "Summary A: {summary_A}\n",
    "Summary B: {summary_B}\n",
    "Based on fluency, which summary do you judge to be better?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payload for the request\n",
    "payload = {\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": \"You are an AI assistant that acts as a judge.\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": fluency_prompt.format(summary_A=summary_1, summary_B=summary_2)\n",
    "    }\n",
    "  ],\n",
    "  \"temperature\": 0.2,\n",
    "  \"top_p\": 0.95,\n",
    "  \"max_tokens\": 800\n",
    "}\n",
    "\n",
    "# Send request\n",
    "try:\n",
    "    response = requests.post(ENDPOINT, headers=headers, json=payload)\n",
    "    response.raise_for_status()  # Will raise an HTTPError if the HTTP request returned an unsuccessful status code\n",
    "except requests.RequestException as e:\n",
    "    raise SystemExit(f\"Failed to make the request. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_response = response.json()\n",
    "summary_1_rating = json_response[\"choices\"][0][\"message\"][\"content\"]\n",
    "print(summary_1_rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LLM has identified summary A as being more fluent for the following reasons:\n",
    "\n",
    "1 - It avoids the unrelated and somewhat informal detail found in Summary B about a 'massive tea party'\n",
    "\n",
    "2 - It identifies the misspelling of summary B.\n",
    "\n",
    "In this small example, the LLM-as-a-Judge has worked very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Further Considerations\n",
    "\n",
    "This notebook aims only to explain the theory behind LLM as a judge.\n",
    "\n",
    "Various considerations should be made before using an LLM as a Judge for evaluation.\n",
    "\n",
    "1 - What metrics are most appropriate? We have shown examples using fluency and groundedness. Other metrics include accuracy, relevance and cohesiveness. \n",
    "\n",
    "2 - Are there pre-existing frameworks that can help you? [DeepEval](https://github.com/confident-ai/deepeval) is one example of an evaluation framework which has a large number of LLM evaluation metrics you can use. \n",
    "\n",
    "3 - Have you followed best practice on prompt guidance? Various [articles](https://www.databricks.com/blog/LLM-auto-eval-best-practices-RAG) offer guidance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
