{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohen Kappa\n",
    "\n",
    "A measure of the inter-rater reliability which takes into account the the possibility of agreement by chance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\kappa = 1 - \\frac{1-P_o}{1-P_e}$\n",
    "\n",
    "Where:\n",
    "- \\( P_o \\) is the **observed agreement**, which is the proportion of times that the raters agree.\n",
    "- \\( P_e \\) is the **expected agreement** by chance, which is the proportion of times that the raters would agree by random chance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider an example where 30 outputs had been assessed by both a human and LLM judge.   Each has been asked to output an objective score of 1 to 5 for the accuracy (taking into account falsehoods and missing content) and a subjective opinion of the clarity of the output (either \"good\", \"okay\" or \"poor\").  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_responses = [\n",
    "    [5, \"good\"], [4, \"good\"], [2, \"okay\"], [5, \"good\"], [2, \"poor\"], \n",
    "    [4, \"good\"], [5, \"good\"], [3, \"okay\"], [4, \"good\"], [5, \"good\"], \n",
    "    [2, \"poor\"], [3, \"okay\"], [5, \"good\"], [4, \"good\"], [3, \"good\"], \n",
    "    [3, \"okay\"], [4, \"good\"], [2, \"poor\"], [4, \"good\"], [5, \"good\"], \n",
    "    [3, \"okay\"], [5, \"good\"], [4, \"good\"], [2, \"poor\"], [4, \"good\"], \n",
    "    [5, \"good\"], [3, \"okay\"], [1, \"good\"], [4, \"good\"], [3, \"okay\"]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_responses = [\n",
    "    [5, \"good\"], [4, \"good\"], [3, \"okay\"], [5, \"good\"], [2, \"poor\"], \n",
    "    [4, \"okay\"], [5, \"good\"], [3, \"okay\"], [4, \"good\"], [5, \"good\"], \n",
    "    [2, \"poor\"], [3, \"okay\"], [5, \"good\"], [4, \"good\"], [5, \"good\"], \n",
    "    [3, \"okay\"], [4, \"good\"], [2, \"poor\"], [4, \"good\"], [5, \"good\"], \n",
    "    [3, \"okay\"], [5, \"good\"], [4, \"good\"], [2, \"poor\"], [4, \"okay\"], \n",
    "    [5, \"good\"], [3, \"okay\"], [5, \"good\"], [4, \"good\"], [3, \"okay\"]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cohen Kappa score for the alignment of the two judges can be calculated using the scikit package.  Here we've decieded to calcualted this separately for the objective component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa Score for Objective Accuracy: 0.8657\n"
     ]
    }
   ],
   "source": [
    "# Extract only the objective accuracy values (first element of each response)\n",
    "human_accuracy = [response[0] for response in human_responses]\n",
    "llm_accuracy = [response[0] for response in llm_responses]\n",
    "\n",
    "# Calculate Cohen's Kappa for objective accuracy\n",
    "kappa_score = cohen_kappa_score(human_accuracy, llm_accuracy)\n",
    "\n",
    "# Output the result\n",
    "print(f\"Cohen's Kappa Score for Objective Accuracy: {kappa_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the subjective componenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa Score for Subjective Accuracy: 0.8795\n"
     ]
    }
   ],
   "source": [
    "# Extract only the subjective accuracy values (first element of each response)\n",
    "human_accuracy_sub = [response[1] for response in human_responses]\n",
    "llm_accuracy_sub = [response[1] for response in llm_responses]\n",
    "\n",
    "# Calculate Cohen's Kappa for subjective accuracy\n",
    "kappa_score_sub = cohen_kappa_score(human_accuracy_sub, llm_accuracy_sub)\n",
    "\n",
    "# Output the result\n",
    "print(f\"Cohen's Kappa Score for Subjective Accuracy: {kappa_score_sub:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A rough interpretation of Cohen's kappa scores:\n",
    "\n",
    "Less than or equal to 0: No agreement\n",
    "0.01–0.20: None to slight agreement\n",
    "0.21–0.40: Fair agreement\n",
    "0.41–0.60: Moderate agreement\n",
    "0.61–0.80: Substantial agreement\n",
    "0.81–1.00: Almost perfect agreement \n",
    "\n",
    "A score of 0 means that the raters agreed as often as if they were randomly guessing. A score of 1 means that the raters agreed completely."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
