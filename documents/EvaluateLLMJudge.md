# Evaluation of LLM as a Judge

Who watches the watchers is a common theme of questioning and can be fractal in nature.  However, in the context of LLM as a judge this needs to be strongly considered as our judge may not have the self-awareness to identify when it’s not up to the task. 
In current literature three main approaches appear to be taken.

## 1.	Comparison to Benchmark
[LLMs instead of Human Judges: A Large Scale Empirical Study across 20 NLP Evaluation Tasks](https://arxiv.org/pdf/2406.18403) compares 11 judges across 20 tasks and shows that when compared to non-expert human evaluators the LLM judges perform better whereas human experts are still superior.   This work created Judge-Bench which could be used for an initial understanding of if a LM judge is giving at least a reasonable performance, but this depends on how closely the final intended task relates to those used in the benchmark.   However, a high variation in the judges was observed across the tasks highlighting that judges need to be assessed against their particular use case rather than against generic averages.

An alternative benchmark is presented by the paper [Evaluating Large Language Models at Evaluating Instruction Following](https://arxiv.org/pdf/2310.07641) which creates the LLM-Bar benchmark.   This benchmark focusses on objective performance beyond correctness through both natural and adversarial human annotation.  The work also includes a discussion of prompting strategies and could be useful for considering any prompt instructions are being assessed in terms of preferencing.

MT-bench is a third benchmark described in [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://arxiv.org/pdf/2306.05685) **DN: More on this**

Overall, all of these benchmarks highlight limitations and are not recommended for specific use-cases but only to support initial development stages and to provide some additional considerations when choosing a model for the judge.

## 2.	Metrics to assess against human evaluations
Alignment metrics are used to compare the evaluations from human judges against LLM judges.  These look at the amount of overlap **DN: Sentence of alignement metrics**

[Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges](https://arxiv.org/pdf/2406.12624v1) uses this approach to compare 9 humans with 9 LLM judges for both base and instruction-tuned evaluations of the TriviaQA dataset (i.e. the task had precise correct answers which could be compared using the alignment approach).    This work concluded that the metric needs to take into account the possibility of chance overlap and thus recommends the Cohen’s kappa metric (or Fleiss’ Kappa metric if there are multiple raters).  The paper concludes with a recommendation that Cohen’s Kappa assessment should be paired with qualitative analysis to be useful.

Developing specific criteria to grade outputs is not an easy task and [Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences](https://arxiv.org/pdf/2404.12272) offers EvalGen as a workflow to support this task.  A key finding of their work is that the evaluation is not independent from the observed outputs and that criteria drifts over time occurs as users grading the outputs develop their criteria as they assess over time. 
Overall, the metric approach is difficult and very bespoke especially for tasks where the LLM judge is attempting to assess none prescribed outputs or when the judging has a subjective component.    For cases where the assessment is objective and can be simplified down to a binary classification task then Cohen’s Kappa is suitable.

## 3.	Identify strategies to improve the judge
[Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://arxiv.org/pdf/2306.05685) & [JudgeLM: Fine-tuned large language models are scalable judges](https://arxiv.org/pdf/2310.17631) identify a range of potential biases that judges can exhibit.   These include:

-	Position bias (**DN: MORE**
-	Verbosity bias (favouring longer more verbose responses)
-	Self-Enhancement bias (prefer answers generated by themselves)
-	Knowledge Bias (**DN: MORE**

[JudgeLM: Fine-tuned large language models are scalable judges](https://arxiv.org/pdf/2310.17631) goes on to recommend mitigations for each of these biases:

-	Swap Augmentation used to mitigate position bias
-	Reference Support to mitigate Knowledge Bias
-	Reference Drop to mitigate format bias

These three mitigations are shown to enhance the judge of interest.

## Additional Considerations
[When combinations of humans and AI are useful: A systematic review and meta-analysis](https://www.nature.com/articles/s41562-024-02024-1) finds an interesting conclusion from a systematic review of 106 studies for performance of evaluations by humans-alone vs AI alone vs human-AI combinations.   It summarised that Human-ai performed worse than the best of human alone or ai alone, but when humans alone outperformed AI alone, they found performance gains in the combination but when AI outperformed humans alone, they found losses.   The also found a trend of negative performance for human-ai synergy for classification tasks whilst a positive uplift in performance for creative tasks.

A couple of additional points that [Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges](https://arxiv.org/pdf/2406.12624v1) makes are that LLM judges appear to judge positively in doubt and struggle mostly with under-specified cases (correct but incomplete answers).

A recommended more detailed consideration of the currently available literature and thinking can be found in the eugeneyan blog called [Evaluating the Effectiveness of LLM-Evaluators (aka LLM-as-Judge)](https://eugeneyan.com/writing/llm-evaluators/) by Ziyou Yan.    One additional component from this write up is the practical consideration of the cost of the judge on the system when this is required as a guardrail and instead suggests that a better approach might be to spend time fine-tuning a classifier or using a reward model.

## In Conclusion
There is no defined or easy way to assess the LLM-as-a-judge.  Instead, a series of comparisons is required during the development phase to test for identified biases and to create a task specific benchmark (including both Cohen Kappa for objective components of the evaluation and human led qualitative analysis focussing on subjective aspects).   This exercise should then be repeated with a frequency depending on the risk of the judge performance being poor balanced against the time taken for the assessment.
